{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 3 : Analyse pour la Construction de la Couche Gold\n",
        "\n",
        "## Objectif\n",
        "\n",
        "Ce notebook vous guide dans l'**analyse des données de la couche silver** pour préparer la construction de la couche **gold** (analytics/BI/ML).\n",
        "\n",
        "La couche gold contient des données transformées, nettoyées et optimisées pour l'analyse métier et la création de rapports/dashboards.\n",
        "\n",
        "## Prérequis\n",
        "\n",
        "Avant d'exécuter ce notebook, assurez-vous d'avoir :\n",
        "\n",
        "1. **Exécuté le notebook `2_[LOAD]_load_to_bigquery.ipynb`** pour avoir toutes les tables dans BigQuery (dataset `silver`)\n",
        "2. **Fichier `.env` configuré** avec les variables d'environnement nécessaires\n",
        "3. **Service Account** avec les permissions BigQuery (`BigQuery Data Viewer`, `BigQuery Job User`)\n",
        "4. **Packages Python installés** : `google-cloud-bigquery`, `pandas`, etc.\n",
        "\n",
        "## Structure du Notebook\n",
        "\n",
        "Ce notebook contient **4 tâches principales** à réaliser :\n",
        "\n",
        "1. **Tâche 1 : Analyser la Granularité** - Comprendre le niveau de détail de chaque table\n",
        "2. **Tâche 2 : Identifier les Transformations** - Déterminer les transformations nécessaires pour la couche gold\n",
        "3. **Tâche 3 : Identifier les Clés de Jointure** - Mapper les relations entre les tables\n",
        "4. **Tâche 4 : Analyse métier Identifier les **KPIs** métiers - \n",
        "\n",
        "## Résultats Attendus\n",
        "\n",
        "À la fin de ce notebook, vous devriez avoir :\n",
        "- Une compréhension claire de la structure et de la granularité de chaque table\n",
        "- Une liste des transformations à appliquer pour créer la couche gold\n",
        "- Un schéma de jointures documenté"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration et Connexion à BigQuery\n",
        "\n",
        "Cette section configure l'environnement et établit la connexion avec BigQuery pour explorer les données de la couche silver.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] - Connecté au projet: idfm-etl-reims-0224dy2025dy\n",
            "[OK] - Dataset: silver\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Third-party imports\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "# Configuration\n",
        "load_dotenv()\n",
        "\n",
        "ROOT = Path.cwd().parent\n",
        "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
        "SA_PATH = ROOT / os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
        "DATASET_ID = \"silver\"\n",
        "\n",
        "# Authentification\n",
        "creds = service_account.Credentials.from_service_account_file(SA_PATH)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID, credentials=creds)\n",
        "\n",
        "print(f\"[OK] - Connecté au projet: {PROJECT_ID}\")\n",
        "print(f\"[OK] - Dataset: {DATASET_ID}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Tâche 1 : Analyser la Granularité de Chaque Table\n",
        "\n",
        "### Objectif\n",
        "\n",
        "La **granularité** d'une table correspond au niveau de détail des données qu'elle contient. Comprendre la granularité est essentiel pour :\n",
        "- Déterminer comment agréger les données\n",
        "- Identifier les duplications potentielles\n",
        "- Comprendre le niveau de détail nécessaire pour les analyses métier\n",
        "\n",
        "### Instructions\n",
        "\n",
        "Pour chaque table du dataset `silver`, vous devez :\n",
        "\n",
        "1. **Lister les colonnes** et leurs types\n",
        "2. **Identifier les clés primaires** ou les colonnes qui identifient de manière unique une ligne\n",
        "3. **Déterminer la granularité** : à quel niveau de détail sont les données ?\n",
        "   - Exemple : `fact_validations` pourrait être au niveau **jour × gare × type de titre**\n",
        "4. **Compter les lignes** et estimer la taille des données\n",
        "5. **Identifier les colonnes de dimension** (références vers d'autres tables)\n",
        "\n",
        "### Exemple de Format de Réponse\n",
        "\n",
        "```\n",
        "Table: dim_gare\n",
        "- Granularité: 1 ligne = 1 gare\n",
        "- Clé primaire: id_gares\n",
        "- Nombre de lignes: 1234\n",
        "- Colonnes de dimension: aucune (table de dimension)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### À Compléter : Analyse de Granularité\n",
        "\n",
        "**Tables de Dimension :**\n",
        "\n",
        "1. `dim_gare`\n",
        "2. `dim_ligne`\n",
        "3. `dim_arret`\n",
        "4. `dim_vacances_scolaires`\n",
        "5. `dim_transporteur`\n",
        "\n",
        "**Tables de Fait :**\n",
        "\n",
        "6. `fact_validations_*` (toutes les tables de validation)\n",
        "\n",
        "**Votre tâche :** Exécutez des requêtes SQL pour analyser chaque table et remplir le tableau ci-dessous.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "BadRequest",
          "evalue": "400 Syntax error: Unexpected identifier \"Votre\" at [2:1]; reason: invalidQuery, location: query, message: Syntax error: Unexpected identifier \"Votre\" at [2:1]\n\nLocation: US\nJob ID: 9268d867-1edd-4a59-8fe4-bcf069dbfec6\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mBadRequest\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Exemple : Analyser la table dim_gare\u001b[39;00m\n\u001b[32m      2\u001b[39m query = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mVotre code ici\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = \u001b[43mbq_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Analyse de dim_gare ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m display(df)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Claudy LINCY\\Desktop\\M2_SEP\\Outils Big data\\m2-univ-reims-sep-cs-etl-sncf-gcp\\venv\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:2159\u001b[39m, in \u001b[36mQueryJob.to_dataframe\u001b[39m\u001b[34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[39m\n\u001b[32m   1929\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_dataframe\u001b[39m(\n\u001b[32m   1930\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1931\u001b[39m     bqstorage_client: Optional[\u001b[33m\"\u001b[39m\u001b[33mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1951\u001b[39m     ] = DefaultPandasDTypes.RANGE_TIMESTAMP_DTYPE,\n\u001b[32m   1952\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mpandas.DataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1953\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[32m   1954\u001b[39m \n\u001b[32m   1955\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2157\u001b[39m \u001b[33;03m            :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[32m   2158\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2159\u001b[39m     query_result = \u001b[43mwait_for_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m query_result.to_dataframe(\n\u001b[32m   2161\u001b[39m         bqstorage_client=bqstorage_client,\n\u001b[32m   2162\u001b[39m         dtypes=dtypes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2176\u001b[39m         range_timestamp_dtype=range_timestamp_dtype,\n\u001b[32m   2177\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Claudy LINCY\\Desktop\\M2_SEP\\Outils Big data\\m2-univ-reims-sep-cs-etl-sncf-gcp\\venv\\Lib\\site-packages\\google\\cloud\\bigquery\\_tqdm_helpers.py:107\u001b[39m, in \u001b[36mwait_for_query\u001b[39m\u001b[34m(query_job, progress_bar_type, max_results)\u001b[39m\n\u001b[32m    103\u001b[39m progress_bar = get_progress_bar(\n\u001b[32m    104\u001b[39m     progress_bar_type, \u001b[33m\"\u001b[39m\u001b[33mQuery is running\u001b[39m\u001b[33m\"\u001b[39m, default_total, \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    105\u001b[39m )\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Claudy LINCY\\Desktop\\M2_SEP\\Outils Big data\\m2-univ-reims-sep-cs-etl-sncf-gcp\\venv\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1773\u001b[39m, in \u001b[36mQueryJob.result\u001b[39m\u001b[34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[39m\n\u001b[32m   1768\u001b[39m     remaining_timeout = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1770\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1771\u001b[39m     \u001b[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[39;00m\n\u001b[32m   1772\u001b[39m     \u001b[38;5;66;03m# long-running API, don't delay the next request at all.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_job_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1774\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1776\u001b[39m     \u001b[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001b[39;00m\n\u001b[32m   1777\u001b[39m     \u001b[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Claudy LINCY\\Desktop\\M2_SEP\\Outils Big data\\m2-univ-reims-sep-cs-etl-sncf-gcp\\venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Claudy LINCY\\Desktop\\M2_SEP\\Outils Big data\\m2-univ-reims-sep-cs-etl-sncf-gcp\\venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Claudy LINCY\\Desktop\\M2_SEP\\Outils Big data\\m2-univ-reims-sep-cs-etl-sncf-gcp\\venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Claudy LINCY\\Desktop\\M2_SEP\\Outils Big data\\m2-univ-reims-sep-cs-etl-sncf-gcp\\venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Claudy LINCY\\Desktop\\M2_SEP\\Outils Big data\\m2-univ-reims-sep-cs-etl-sncf-gcp\\venv\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1722\u001b[39m, in \u001b[36mQueryJob.result.<locals>.is_job_done\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1699\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m job_failed_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1700\u001b[39m     \u001b[38;5;66;03m# Only try to restart the query job if the job failed for\u001b[39;00m\n\u001b[32m   1701\u001b[39m     \u001b[38;5;66;03m# a retriable reason. For example, don't restart the query\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1719\u001b[39m     \u001b[38;5;66;03m# into an exception that can be processed by the\u001b[39;00m\n\u001b[32m   1720\u001b[39m     \u001b[38;5;66;03m# `job_retry` predicate.\u001b[39;00m\n\u001b[32m   1721\u001b[39m     restart_query_job = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1722\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m job_failed_exception\n\u001b[32m   1723\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1724\u001b[39m     \u001b[38;5;66;03m# Make sure that the _query_results are cached so we\u001b[39;00m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;66;03m# can return a complete RowIterator.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1731\u001b[39m     \u001b[38;5;66;03m# making any extra API calls if the previous loop\u001b[39;00m\n\u001b[32m   1732\u001b[39m     \u001b[38;5;66;03m# iteration fetched the finished job.\u001b[39;00m\n\u001b[32m   1733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reload_query_results(\n\u001b[32m   1734\u001b[39m         retry=retry, **reload_query_results_kwargs\n\u001b[32m   1735\u001b[39m     )\n",
            "\u001b[31mBadRequest\u001b[39m: 400 Syntax error: Unexpected identifier \"Votre\" at [2:1]; reason: invalidQuery, location: query, message: Syntax error: Unexpected identifier \"Votre\" at [2:1]\n\nLocation: US\nJob ID: 9268d867-1edd-4a59-8fe4-bcf069dbfec6\n"
          ]
        }
      ],
      "source": [
        "# Exemple : Analyser la table dim_gare\n",
        "query = f\"\"\"\n",
        "Votre code ici\n",
        "\"\"\"\n",
        "\n",
        "df = bq_client.query(query).to_dataframe()\n",
        "print(\"=== Analyse de dim_gare ===\")\n",
        "display(df)\n",
        "\n",
        "# Afficher le schéma\n",
        "table = bq_client.get_table(f\"{PROJECT_ID}.{DATASET_ID}.dim_gare\")\n",
        "print(\"\\n=== Schéma ===\")\n",
        "for field in table.schema:\n",
        "    print(f\"  - {field.name}: {field.field_type} ({field.mode})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Tâche 2 : Identifier les Transformations Nécessaires\n",
        "\n",
        "### Objectif\n",
        "\n",
        "Identifier les **transformations** à appliquer aux données de la couche silver pour créer la couche gold optimisée pour l'analyse.\n",
        "\n",
        "### Types de Transformations Possibles\n",
        "\n",
        "1. **Nettoyage des données**\n",
        "   - Suppression des doublons\n",
        "   - Gestion des valeurs NULL\n",
        "   - Normalisation des formats (dates, textes)\n",
        "\n",
        "2. **Enrichissement**\n",
        "   - Ajout de colonnes calculées\n",
        "   - Jointures avec les tables de dimension\n",
        "   - Ajout de catégories/segments\n",
        "\n",
        "3. **Agrégation**\n",
        "   - Regroupement par dimensions (jour, gare, ligne, etc.)\n",
        "   - Calcul de métriques (somme, moyenne, comptage)\n",
        "   - Création de tables pré-agrégées\n",
        "\n",
        "### Instructions\n",
        "\n",
        "Pour chaque table, identifiez :\n",
        "1. **Les problèmes de qualité** à corriger\n",
        "2. **Les transformations nécessaires** avec des exemples concrets\n",
        "3. **Les colonnes à ajouter** (calculées ou issues de jointures)\n",
        "4. **Les agrégations possibles** pour optimiser les requêtes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Exemple : Recherche de doublons ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/admin/.pyenv/versions/3.11.14/envs/etl-gcp/lib/python3.11/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>JOUR</th>\n",
              "      <th>ID_ZDC</th>\n",
              "      <th>CATEGORIE_TITRE</th>\n",
              "      <th>nb_occurrences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-11-03</td>\n",
              "      <td>999999</td>\n",
              "      <td>Amethyste</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-11-06</td>\n",
              "      <td>999999</td>\n",
              "      <td>Amethyste</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-11-08</td>\n",
              "      <td>999999</td>\n",
              "      <td>Amethyste</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-11-09</td>\n",
              "      <td>999999</td>\n",
              "      <td>Amethyste</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-11-12</td>\n",
              "      <td>999999</td>\n",
              "      <td>Amethyste</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2023-11-14</td>\n",
              "      <td>999999</td>\n",
              "      <td>Amethyste</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2023-11-17</td>\n",
              "      <td>999999</td>\n",
              "      <td>Amethyste</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2023-07-01</td>\n",
              "      <td>71379</td>\n",
              "      <td>Amethyste</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2023-07-02</td>\n",
              "      <td>71379</td>\n",
              "      <td>Amethyste</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2023-07-03</td>\n",
              "      <td>71379</td>\n",
              "      <td>Amethyste</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         JOUR  ID_ZDC CATEGORIE_TITRE  nb_occurrences\n",
              "0  2023-11-03  999999       Amethyste               2\n",
              "1  2023-11-06  999999       Amethyste               2\n",
              "2  2023-11-08  999999       Amethyste               2\n",
              "3  2023-11-09  999999       Amethyste               2\n",
              "4  2023-11-12  999999       Amethyste               2\n",
              "5  2023-11-14  999999       Amethyste               2\n",
              "6  2023-11-17  999999       Amethyste               2\n",
              "7  2023-07-01   71379       Amethyste               2\n",
              "8  2023-07-02   71379       Amethyste               2\n",
              "9  2023-07-03   71379       Amethyste               2"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Exemple 1 : Vérifier les doublons dans fact_validations\n",
        "query = f\"\"\"\n",
        "Votre code ici\n",
        "\"\"\"\n",
        "\n",
        "df_duplicates = bq_client.query(query).to_dataframe()\n",
        "print(\"=== Exemple : Recherche de doublons ===\")\n",
        "display(df_duplicates)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Tâche 3 : Identifier les Clés de Jointure Possibles\n",
        "\n",
        "### Objectif\n",
        "\n",
        "Identifier toutes les **relations possibles** entre les tables pour pouvoir créer des jointures dans la couche gold.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "Pour chaque paire de tables, identifiez :\n",
        "\n",
        "1. **Les colonnes de jointure** (clés étrangères)\n",
        "2. **Le type de relation** (1-1, 1-N, N-N)\n",
        "3. **La cardinalité** (combien de lignes de la table A correspondent à combien de lignes de la table B)\n",
        "4. **Vérifier l'intégrité référentielle** (toutes les clés étrangères existent-elles dans la table de dimension ?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Tâche 4 : Analyse métier\n",
        "\n",
        "Identifier les **KPIs** (Key Performance Indicators) à calculer :\n",
        "- Nombre total de validations par période\n",
        "- Répartition par type de titre\n",
        "- Top 10 des gares les plus fréquentées\n",
        "- Comparaison jour ouvrable vs weekend\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
